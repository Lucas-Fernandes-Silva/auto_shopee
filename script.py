# #Criar um script que fa√ßa:  
# #Pegue as nfe em xml do email  
# #Extrai os dados necessarios   
# #Tratar dados   
# #Remover materiais pesado E outras notas 
# -TABUAS CABOQUINHO  
# -CIMENTRO VOTORAN  
# -Magalu   
# 
# #Aplicador formula de taxas  
# #Inserir no template dos marketplaces
#   

import os
import shutil
import xml.etree.ElementTree as ET
import pandas as pd
from imap_tools import MailBox, AND
from config import pwd, user
from datetime import date
import requests
from bs4 import BeautifulSoup
import json
from playwright.async_api import async_playwright
import asyncio


save_folder = "notas/nfes/"
os.makedirs(save_folder, exist_ok=True)

# üîπ Baixar anexos XML do e-mail
with MailBox("imap.gmail.com").login(user, pwd, initial_folder="INBOX") as mailbox:
    list_mail = mailbox.fetch(criteria=AND(date=date.today()))
    for email in list_mail:
        for anexo in email.attachments:
            if anexo.filename.lower().endswith(".xml"):
                file_path = os.path.join(save_folder, anexo.filename)
                if not os.path.exists(file_path):
                    with open(file_path, "wb") as f:
                        f.write(anexo.payload)

pasta_origem = "notas/nfes"
todos_produtos = []


# üîπ Fun√ß√£o para extrair dados do XML
def extrai_dados(caminho_arquivo):
    produtos = []
    tree = ET.parse(caminho_arquivo)
    root = tree.getroot()
    ns = {"ns": "http://www.portalfiscal.inf.br/nfe"}

    natOp = root.find(".//ns:ide/ns:natOp", ns)
    if natOp is not None and natOp.text.strip().upper() == 'BONIFICACAO, DOACAO OU BRINDE':
        return [] 

    for det in root.findall(".//ns:det", ns):   
        produto = {}
        produto['Codigo Produto'] = det.find("./ns:prod/ns:cProd", ns).text
        produto['Descri√ß√£o'] = det.find("./ns:prod/ns:xProd", ns).text
        produto['Valor_unit√°rio'] = det.find("./ns:prod/ns:vUnCom", ns).text
        produto['C√≥digo de Barras'] = det.find("./ns:prod/ns:cEAN", ns).text 
        produto['Sku'] = produto['C√≥digo de Barras']
        if produto['Sku'] == 'SEM GTIN':
            produto['Sku'] = produto['Descri√ß√£o']
        produto['Fornecedor'] = root.find(".//ns:emit/ns:xNome", ns).text
        produtos.append(produto)
    return produtos
        

# üîπ Processar todos os XMLs da pasta
for arquivo in os.listdir(pasta_origem):
    if arquivo.lower().endswith(".xml"):
        caminho_arquivo = os.path.join(pasta_origem, arquivo)
        try:
            tree = ET.parse(caminho_arquivo)
            root = tree.getroot()
            ns = {"ns": "http://www.portalfiscal.inf.br/nfe"}

            cnpj_emitente = root.find(".//ns:emit/ns:CNPJ", ns)
            if cnpj_emitente is None:
                print(f"‚ö†Ô∏è CNPJ n√£o encontrado em {arquivo}")
                continue

            produtos_xml = extrai_dados(caminho_arquivo)
            todos_produtos.extend(produtos_xml)

            # Copia o arquivo para "notas/"
            pasta_destino = os.path.join("notas/")
            os.makedirs(pasta_destino, exist_ok=True)
            shutil.copy(caminho_arquivo, pasta_destino)

        except Exception as e:
            print(f"{arquivo}: {e}")


# üîπ Fornecedores que devem ser ignorados
fornecedores_pesados = [
    'IND E COM DE TUBOS E CONEXOES FORT.COM',
    'VOTORANTIM CIMENTOS SA',
    'CABOQUINHO MATERIAIS PARA CONSTRUCAO'
]

# üîπ DataFrame inicial
produtos = pd.DataFrame(todos_produtos)
produtos = produtos[~produtos['Fornecedor'].isin(fornecedores_pesados)]
produtos = produtos.drop_duplicates(subset='Codigo Produto', keep='first')

# üîπ Headers para requests
headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 "
                  "(KHTML, like Gecko) Chrome/139.0.0.0 Safari/537.36",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8",
    "Accept-Language": "pt-BR,pt;q=0.9,en-US;q=0.8,en;q=0.7",
}


# üîπ Fun√ß√£o fallback com Playwright
async def get_data_playwright(url):
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        page = await browser.new_page()
        await page.goto(url, wait_until="networkidle")

        html = await page.content()
        soup = BeautifulSoup(html, "html.parser")
        script = soup.find("script", id="__NEXT_DATA__")
        if not script:
            await browser.close()
            return {}

        data = json.loads(script.string)
        produto = data.get("props", {}).get("pageProps", {}).get("produto", {})
        url_img = data.get("props", {}).get("pageProps", {}).get("seo", {}).get("imageUrl", 'N√£o dispon√≠vel')

        await browser.close()
        return {
            'marca': next((p.get("desc") for p in produto.get("dimensoes", []) if p.get("label") == "MARCA"), "N√£o dispon√≠vel"),
            'peso': produto.get("pesoBruto", "N√£o dispon√≠vel"),
            'codigo_barras': produto.get("codBarra", "N√£o dispon√≠vel"),
            'url_img': url_img,
        }


# üîπ Loop principal (s√≠ncrono, com fallback)
def processa_produtos(produtos, headers):
    for index, produto in produtos.iterrows():
        fornecedor = produto['Fornecedor']
        codigo_produto = produto['Codigo Produto']
        descricao = produto['Descri√ß√£o']

        # Monta URL conforme fornecedor
        if fornecedor == 'CONSTRUDIGI DISTRIBUIDORA DE MATERIAIS PARA CONSTRUCAO LTDA':   
            url = f'https://www.construdigi.com.br/produto/{codigo_produto}/{descricao}'
        elif fornecedor == 'M.S.B. COMERCIO DE MATERIAIS PARA CONSTRUCAO':
            url = f'https://msbitaqua.com.br/produto/{codigo_produto}/{descricao}'
        elif fornecedor == "CONSTRUJA DISTR. DE MATERIAIS P/ CONSTRU":
            url = f'https://www.construja.com.br/produto/{codigo_produto}/{descricao}'
        else:
            print('Indispon√≠vel')
            continue

        try:
            # Primeiro tenta com requests (r√°pido)
            response = requests.get(url, headers=headers, timeout=10)
            soup = BeautifulSoup(response.text, "html.parser")
            script = soup.find("script", type="application/json")
            data = json.loads(script.string) if script else {}

            extrai = data.get("props", {}).get("pageProps", {}).get("produto", {})
            url_img = data.get("props", {}).get("pageProps", {}).get("seo", {}).get("imageUrl", '')

            marca = next((p.get("desc") for p in extrai.get("dimensoes", []) if p.get("label") == "MARCA"), "")
            peso = extrai.get("pesoBruto", "")
            codigo_barras = extrai.get("codBarra", "")

            # Se n√£o conseguiu pegar nada ‚Üí fallback Playwright
            if not (marca or peso or codigo_barras):
                print(f"‚ö†Ô∏è Fallback Playwright para {descricao}")
                dados = asyncio.run(get_data_playwright(url))
                marca = dados["marca"]
                peso = dados["peso"]
                codigo_barras = dados["codigo_barras"]
                url_img = dados["url_img"]

            # Atualiza DataFrame
            produtos.at[index, 'C√≥digo de Barras'] = codigo_barras or "N√£o dispon√≠vel"
            produtos.at[index, 'Peso'] = peso or "N√£o dispon√≠vel"
            produtos.at[index, "Marca"] = marca or "N√£o dispon√≠vel"
            produtos.at[index, 'Url Imagem'] = url_img or "N√£o dispon√≠vel"

            print("‚úÖ", descricao)

        except Exception as e:
            print("‚ùå Erro:", e)


# üîπ Executar
processa_produtos(produtos, headers)

# üîπ Salvar Excel final
produtos.to_excel("produtos.xlsx", index=False)
print("üìÅ Arquivo 'produtos.xlsx' gerado com sucesso!")
