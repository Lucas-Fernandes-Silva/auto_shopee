{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4edf798",
   "metadata": {},
   "source": [
    "#Criar um script que fa√ßa:  \n",
    "#Pegue as nfe em xml do email  \n",
    "#Extrai os dados necessarios   \n",
    "#Tratar dados   \n",
    "#Remover materiais pesado E outras notas \n",
    "-TABUAS CABOQUINHO  \n",
    "-CIMENTRO VOTORAN  \n",
    "-Magalu   \n",
    "\n",
    "#Aplicador formula de taxas  \n",
    "#Inserir no template dos marketplaces\n",
    "\n",
    "Ultilizar api do bling para cadastro de itens   (quando tiver)\n",
    "\n",
    "\n",
    "Mercado LIVRE:50% do valor do produto (para produtos at√© R$ 12,50), R$ 6,25 (entre R$ 12,50 e R$ 29), R$ 6,50 (entre R$ 29 e R$ 50) e R$ 6,75 (entre R$ 50 e R$ 79)\n",
    "\n",
    "Comiss√£o de Categoria Constru√ß√£o 12%\n",
    "\n",
    "\n",
    "SHOPE taxa de transa√ß√£o √© sobre o total do produto ent√£o √© 2% de taxa e 18 de comiss√£o\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14fbee72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "from imap_tools import MailBox, AND\n",
    "from env import pwd, user\n",
    "from datetime import date, datetime\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7f5115",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_folder = \"notas/nfes/\"\n",
    "os.makedirs(save_folder, exist_ok=True)\n",
    "\n",
    "with MailBox(\"imap.gmail.com\").login(\n",
    "    user,\n",
    "    pwd,\n",
    "    initial_folder=\"INBOX\",\n",
    ") as mailbox:\n",
    "    list_mail = mailbox.fetch(criteria=AND(date=date.today()))\n",
    "    for email in list_mail:\n",
    "        for anexo in email.attachments:\n",
    "            if anexo.filename.lower().endswith(\".xml\"):\n",
    "                file_path = os.path.join(save_folder, anexo.filename)\n",
    "                if not os.path.exists(file_path):\n",
    "                    with open(file_path, \"wb\") as f:\n",
    "                        f.write(anexo.payload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7e382e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pasta_origem = \"notas/nfes\"\n",
    "todos_produtos = []\n",
    "\n",
    "\n",
    "def extrai_dados(caminho_arquivo):\n",
    "    produtos = []\n",
    "    tree = ET.parse(caminho_arquivo)\n",
    "    root = tree.getroot()\n",
    "    ns = {\"ns\": \"http://www.portalfiscal.inf.br/nfe\"}\n",
    "    data_str = root.find(\".//ns:ide/ns:dhEmi\", ns).text\n",
    "    data_emissao = datetime.fromisoformat(data_str)\n",
    "\n",
    "    natOp = root.find(\".//ns:ide/ns:natOp\", ns)\n",
    "    if (\n",
    "        natOp is not None\n",
    "        and natOp.text.strip().upper() == \"BONIFICACAO, DOACAO OU BRINDE\"\n",
    "    ):\n",
    "        return []\n",
    "\n",
    "    for det in root.findall(\".//ns:det\", ns):\n",
    "        produto = {}\n",
    "        temp_codigo = det.find(\"./ns:prod/ns:cProd\", ns).text\n",
    "        if \"-\" not in temp_codigo:\n",
    "            produto[\"Codigo Produto\"] = temp_codigo\n",
    "        produto[\"Descri√ß√£o\"] = det.find(\"./ns:prod/ns:xProd\", ns).text\n",
    "        produto[\"Valor_unit√°rio\"] = det.find(\"./ns:prod/ns:vUnCom\", ns).text\n",
    "        produto[\"C√≥digo de Barras\"] = det.find(\"./ns:prod/ns:cEAN\", ns).text\n",
    "        produto[\"Sku\"] = produto[\"C√≥digo de Barras\"]\n",
    "        if produto[\"Sku\"] == \"SEM GTIN\":\n",
    "            produto[\"Sku\"] = produto[\"Descri√ß√£o\"]\n",
    "        produto[\"Fornecedor\"] = root.find(\".//ns:emit/ns:xNome\", ns).text\n",
    "        produto[\"Data Emiss√£o\"] = data_emissao\n",
    "\n",
    "        produtos.append(produto)\n",
    "    return produtos\n",
    "\n",
    "\n",
    "# Percorre todos os arquivos da pasta\n",
    "for arquivo in os.listdir(pasta_origem):\n",
    "    if arquivo.lower().endswith(\".xml\"):\n",
    "        caminho_arquivo = os.path.join(pasta_origem, arquivo)\n",
    "\n",
    "        try:\n",
    "            # Abre e parseia o XML\n",
    "            tree = ET.parse(caminho_arquivo)\n",
    "            root = tree.getroot()\n",
    "\n",
    "            # Define namespace\n",
    "            ns = {\"ns\": \"http://www.portalfiscal.inf.br/nfe\"}\n",
    "\n",
    "            # Busca o CNPJ do emitente\n",
    "            cnpj_emitente = root.find(\".//ns:emit/ns:CNPJ\", ns)\n",
    "            cnpj_emitente\n",
    "            if cnpj_emitente is None:\n",
    "                print(f\"‚ö†Ô∏è CNPJ n√£o encontrado em {arquivo}\")\n",
    "                continue\n",
    "\n",
    "            cnpj_emitente = cnpj_emitente.text\n",
    "\n",
    "            produtos = extrai_dados(caminho_arquivo)\n",
    "            todos_produtos.extend(produtos)\n",
    "\n",
    "            emitente = root.find(\".//ns:emit/ns:xNome\", ns).text\n",
    "\n",
    "            pasta_destino = os.path.join(\"notas/\")\n",
    "\n",
    "            # Cria a pasta se n√£o existir\n",
    "            os.makedirs(pasta_destino, exist_ok=True)\n",
    "\n",
    "            # Copia o arquivo para a pasta\n",
    "            shutil.copy(caminho_arquivo, pasta_destino)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\" {arquivo}: {e}\")\n",
    "\n",
    "\n",
    "fornecedores_pesados = [\n",
    "    \"IND E COM DE TUBOS E CONEXOES FORT.COM\",\n",
    "    \"VOTORANTIM CIMENTOS SA\",\n",
    "    \"CABOQUINHO MATERIAIS PARA CONSTRUCAO\",\n",
    "]\n",
    "\n",
    "\n",
    "produtos = pd.DataFrame(todos_produtos)\n",
    "produtos = produtos[~produtos[\"Fornecedor\"].isin(fornecedores_pesados)]\n",
    "produtos = produtos.sort_values(by=\"Data Emiss√£o\", ascending=False)\n",
    "\n",
    "produtos = produtos.drop_duplicates(subset=\"Codigo Produto\", keep=\"first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4337959b",
   "metadata": {},
   "outputs": [],
   "source": [
    "produtos[\"Fornecedor\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f384208",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \"\n",
    "    \"(KHTML, like Gecko) Chrome/139.0.0.0 Safari/537.36\",\n",
    "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8\",\n",
    "    \"Accept-Language\": \"pt-BR,pt;q=0.9,en-US;q=0.8,en;q=0.7\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d981c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "read = pd.read_excel(\"upseller_template.xlsx\")\n",
    "produtos = read[-1:]\n",
    "\n",
    "\n",
    "for index, produto in produtos.iterrows():\n",
    "    # fornecedor = produto['Fornecedor']\n",
    "    # codigo_produto = produto['Codigo Produto']\n",
    "    # descricao = produto['Descri√ß√£o']\n",
    "    try:\n",
    "        #     if fornecedor == 'CONSTRUDIGI DISTRIBUIDORA DE MATERIAIS PARA CONSTRUCAO LTDA':\n",
    "        #         url = f'https://www.construdigi.com.br/produto/{codigo_produto}/{descricao}'\n",
    "        #     elif fornecedor == 'M.S.B. COMERCIO DE MATERIAIS PARA CONSTRUCAO':\n",
    "        #         url = f'https://msbitaqua.com.br/produto/{codigo_produto}/{descricao}'\n",
    "        #     elif fornecedor == \"CONSTRUJA DISTR. DE MATERIAIS P/ CONSTRU\":\n",
    "        #         url = f'https://www.construja.com.br/produto/{codigo_produto}/{descricao}'\n",
    "        #     else:\n",
    "        #         print('Indisponivel')\n",
    "        #         continue\n",
    "\n",
    "        url = \"https://construja.com.br/produto/101223/estrela-tapa-furo-ll-20mm\"\n",
    "\n",
    "        response = requests.get(url, headers=headers)\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        script = soup.find(\"script\", type=\"application/json\")\n",
    "        data = json.loads(script.string) if script else {}\n",
    "\n",
    "        extrai = data.get(\"props\", {}).get(\"pageProps\", {}).get(\"produto\", {})\n",
    "        comprimento = extrai.get(\"comprimento\")\n",
    "\n",
    "        url_img = (\n",
    "            data.get(\"props\", {})\n",
    "            .get(\"pageProps\", {})\n",
    "            .get(\"seo\", {})\n",
    "            .get(\"imageUrl\", \"N√£o disponivel\")\n",
    "        )\n",
    "        marca = next(\n",
    "            (\n",
    "                p.get(\"desc\")\n",
    "                for p in extrai.get(\"dimensoes\", [])\n",
    "                if p.get(\"label\") == \"MARCA\"\n",
    "            ),\n",
    "            \"N√£o dispon√≠vel\",\n",
    "        )\n",
    "        categoria = next(\n",
    "            (\n",
    "                p.get(\"desc\")\n",
    "                for p in extrai.get(\"dimensoes\", [])\n",
    "                if p.get(\"label\") == \"SUB CATEGORIA\"\n",
    "            ),\n",
    "            \"N√£o dispon√≠vel\",\n",
    "        )\n",
    "        print(categoria)\n",
    "        peso = extrai.get(\"pesoBruto\", \"N√£o dispon√≠vel\")\n",
    "        if codigo_barras == \"SEM GTIN\":\n",
    "            codigo_barras = extrai.get(\"codBarra\", \"SEM GTIN\")\n",
    "        produtos.at[index, \"C√≥digo de Barras\"] = codigo_barras\n",
    "        produtos.at[index, \"Peso\"] = peso\n",
    "        produtos.at[index, \"Marca\"] = marca\n",
    "        produtos.at[index, \"Url Imagem\"] = url_img\n",
    "\n",
    "        print(produto[\"Descri√ß√£o\"])\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d33c92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "# === 1. Carregar o arquivo ===\n",
    "df = pd.read_excel(\"produtos.xlsx\")\n",
    "\n",
    "\n",
    "# === 2. Fun√ß√£o para limpar e normalizar texto ===\n",
    "def limpar_texto(txt):\n",
    "    if not isinstance(txt, str):\n",
    "        return \"\"\n",
    "    # Remove acentos\n",
    "    txt = unicodedata.normalize(\"NFD\", txt)\n",
    "    txt = txt.encode(\"ascii\", \"ignore\").decode(\"utf-8\")\n",
    "    # Converte para mai√∫sculas\n",
    "    txt = txt.upper()\n",
    "    # Remove caracteres especiais\n",
    "    txt = re.sub(r\"[^A-Z0-9 ]\", \"\", txt)\n",
    "    # Remove espa√ßos duplicados\n",
    "    txt = re.sub(r\"\\s+\", \" \", txt).strip()\n",
    "    return txt\n",
    "\n",
    "\n",
    "# === 3. Lista de marcas fixas (for√ßadas a fazer parte da an√°lise) ===\n",
    "marcas_adicionais = [\n",
    "    \"SHIVA\",\n",
    "    \"BRASCOLA\",\n",
    "    \"FERTAK\",\n",
    "    \"NATICON\",\n",
    "    \"NOBRE\",\n",
    "    \"ADELBRAS\",\n",
    "    \"STA MARINA\",\n",
    "    \"FIRMEZA\",\n",
    "    \"OLIPLAS\",\n",
    "    \"STAM\",\n",
    "    \"R.C.A\",\n",
    "    \"CORONA\",\n",
    "    \"TOP FIO\",\n",
    "    \"MAKITA\",\n",
    "    \"REXON\",\n",
    "    \"SOPRANO\",\n",
    "    \"FAST LUB\",\n",
    "    \"ACQUA\",\n",
    "    \"KIAN\",\n",
    "    \"SOUDAL\",\n",
    "    \"DEPLAST\",\n",
    "    \"TATU\",\n",
    "    \"GRENDHA\",\n",
    "    \"PANASONIC\",\n",
    "    \"PAULICEIA\",\n",
    "    \"MORIA\",\n",
    "    \"ALLTAPE\",\n",
    "    \"PLASTILIT\",\n",
    "    \"TITANIUM\",\n",
    "    \"FERRARI\",\n",
    "    \"NEGREIRA\",\n",
    "    \"RADIAL\",\n",
    "    \"BLUMENAU\",\n",
    "    \"J.F METAIS\",\n",
    "    \"ART METAIS\",\n",
    "    \"LONAX\",\n",
    "    \"GHEL PLUS\",\n",
    "    \"CHEMICOLOR\",\n",
    "    \"SANY\",\n",
    "    \"PROAQUA\",\n",
    "    \"BICROM\",\n",
    "    \"WAGO\",\n",
    "    \"TECHNA\",\n",
    "    \"METASUL\",\n",
    "    \"GARAPEIRA\",\n",
    "    \"ROCO\",\n",
    "    \"PRO FOAM\",\n",
    "    \"DURACELL\",\n",
    "    \"POLIBEL\",\n",
    "    \"AGELUX\",\n",
    "    \"REMOX\",\n",
    "    \"RORATO\",\n",
    "    \"WAVES\",\n",
    "    \"LEGRAND\",\n",
    "    \"PAPAIZ\",\n",
    "    \"AGELUZ\",\n",
    "    \"IMPERIAL\",\n",
    "    \"HYDRA\",\n",
    "    \"KITUBO\",\n",
    "    \"NETTE\",\n",
    "    \"SANLIMP\",\n",
    "    \"EMA METAIS\",\n",
    "    \"M&M\" \"BEARS\",\n",
    "    \"ADTEX\",\n",
    "    \"GALO\",\n",
    "    \"PEGAFORTE\",\n",
    "    \"EMAVE\",\n",
    "    \"ELGIN\",\n",
    "    \"LUCONI\",\n",
    "    \"VEGA\",\n",
    "    \"MUNDIAL\",\n",
    "    \"DELTA\",\n",
    "    \"COBRECOM\",\n",
    "    \"PLASTBIG\",\n",
    "    \"FERE\",\n",
    "    \"SATA\",\n",
    "    \"ALUMBRA\",\n",
    "    \"FIOLUX\",\n",
    "    \"PACETTA\",\n",
    "    \"ITAMBE\",\n",
    "    \"PEXCEL\",\n",
    "    \"EMAVA\",\n",
    "    \"KADESH\",\n",
    "    \"IBERE\",\n",
    "    \"GOLD\",\n",
    "    \"PERLEX\",\n",
    "    \"IMA\",\n",
    "    \"SAFETY\",\n",
    "    \"PIAL\",\n",
    "    \"PLASTIK\",\n",
    "    \"OUROLUX\",\n",
    "    \"LED BEE\",\n",
    "    \"BELZER\",\n",
    "    \"PROTEG\",\n",
    "    \"NASTRO\",\n",
    "    \"FJ FERR\",\n",
    "    \"SAO RAFAEL\",\n",
    "    \"STILLUS\",\n",
    "    \"FLASH LIMP\",\n",
    "    \"ZUMPLAST\",\n",
    "    \"REDY\",\n",
    "    \"PRIMETECH\",\n",
    "    \"SECALUX\",\n",
    "    \"PLASTIC\",\n",
    "    \"SUPER BONDER\",\n",
    "    \"ARTPLAS\",\n",
    "    \"FABER CASTELL\",\n",
    "    \"GUARANI\",\n",
    "    \"DEWALT\",\n",
    "    \"ALUREM\",\n",
    "    \"PERFIX \",\n",
    "    \"STELL\",\n",
    "    \"FERROX\",\n",
    "    \"USAF\",\n",
    "    \"INJET\",\n",
    "    \"CMK\",\n",
    "    \"BEARS\",\n",
    "    \"MINASUL\",\n",
    "    \"DEGOMASTER\",\n",
    "    \"SIENA\",\n",
    "]\n",
    "marcas_adicionais = [limpar_texto(m) for m in marcas_adicionais]\n",
    "\n",
    "\n",
    "# === 4. Dicion√°rio de sin√¥nimos / varia√ß√µes ===\n",
    "mapa_variacoes = {\n",
    "    \"TRAMONTINA\": [\n",
    "        \"TRAMON\",\n",
    "        \"TRAMONTINA\",\n",
    "        \"TRAMONTINA ELE\",\n",
    "        \"TRAMONTINA GARI\",\n",
    "        \"TRAMONTINA MULT\",\n",
    "        \"TRAM\",\n",
    "    ],\n",
    "    \"JOMARCA\": [\"JOMARCA\", \"JOMARC\", \"JOMARCA JMC\", \"JMC\"],\n",
    "    \"NEW FIX\": [\n",
    "        \"NEW FIX\",\n",
    "        \"NEWFIX\",\n",
    "        \"NEW-FIX\",\n",
    "        \"NEW F\",\n",
    "        \"PARAF CHIPBOARD CHATA PHS\",\n",
    "        \"PARAF SEX.ROS.SOBERBA\",\n",
    "    ],\n",
    "    \"NOVA\": [\"NOVA TINTAS\", \"NOVA\"],\n",
    "    \"DRYKO\": [\"DRYKO\", \"DRYKOPRIMER\"],\n",
    "    \"SIKA\": [\"SIKATOP\", \"SIKA\"],\n",
    "    \"BRASFORT\": [\"BRASF\", \"BRASFORT\"],\n",
    "    \"ILUMI\": [\"ILUM\", \"ILUMI\"],\n",
    "    \"DENVER\": [\n",
    "        \"DENVER\",\n",
    "        \"DENVERIMPER\",\n",
    "        \"DENVERLAJE\",\n",
    "        \"DENVERTEC\",\n",
    "        \"DENVERCRILL\",\n",
    "        \"DENVERFITA\",\n",
    "        \"DENVERFIX\",\n",
    "    ],\n",
    "    \"THOMPSOM\": [\"THOMPSOM\", \"THOMP\"],\n",
    "    \"PEGAFORTE\": [\"PEGAFORTE\", \"PEGA FORTE\"],\n",
    "    \"CALHA FORTE\": [\"CALHA FORTE\", \"CALHA FORT\"],\n",
    "    \"VEDACIT\": [\"VEDACIT\", \"BIANCO\"],\n",
    "    \"LORENZETTI\": [\"LOREN\", \"LORENZETTI\", \"ORIG.L&C D\", \".PMR D.\", \".L&C \"],\n",
    "    \"KADESH\": [\"BOT FLEX ELASTICO PRETA BI\"],\n",
    "}\n",
    "\n",
    "\n",
    "# === 5. Montar lista final de marcas conhecidas ===\n",
    "marcas_planilha = df[\"Marca\"].dropna().unique().tolist()\n",
    "marcas_planilha.remove(\"5+\")\n",
    "marcas_planilha = [\n",
    "    limpar_texto(m) for m in marcas_planilha if isinstance(m, str) and m.strip()\n",
    "]\n",
    "\n",
    "# Combinar todas\n",
    "marcas = list(set(marcas_planilha + marcas_adicionais))\n",
    "\n",
    "# Adicionar tamb√©m as varia√ß√µes do mapa\n",
    "for marca_principal, variacoes in mapa_variacoes.items():\n",
    "    marcas.append(marca_principal)\n",
    "    marcas.extend([limpar_texto(v) for v in variacoes])\n",
    "\n",
    "marcas = list(set(marcas))\n",
    "\n",
    "\n",
    "# === 7. Fun√ß√£o para detectar marca na descri√ß√£o ===\n",
    "def detectar_marca(descricao):\n",
    "    desc_limpa = limpar_texto(descricao)\n",
    "\n",
    "    # Verifica varia√ß√µes conhecidas primeiro\n",
    "    for marca_padrao, variacoes in mapa_variacoes.items():\n",
    "        for v in variacoes:\n",
    "            if limpar_texto(v) in desc_limpa:\n",
    "                return marca_padrao\n",
    "\n",
    "    # Caso n√£o encontre nas varia√ß√µes, busca direta na lista geral\n",
    "    for marca in marcas:\n",
    "        if marca and marca in desc_limpa:\n",
    "            return marca\n",
    "\n",
    "    return np.nan\n",
    "\n",
    "\n",
    "df_filtrado = df.copy()\n",
    "\n",
    "# === 8. Detectar e preencher marcas ausentes ===\n",
    "df_filtrado[\"Marca Detectada\"] = df_filtrado[\"Descri√ß√£o\"].apply(detectar_marca)\n",
    "\n",
    "# Preenche apenas onde est√° vazio\n",
    "df_filtrado[\"Marca\"] = df_filtrado[\"Marca\"].fillna(df_filtrado[\"Marca Detectada\"])\n",
    "\n",
    "df_filtrado[\"Marca\"] = df_filtrado[\"Marca\"].fillna(\"GEN√âRICO\")\n",
    "# Atualiza no dataframe principal\n",
    "df.update(df_filtrado)\n",
    "\n",
    "\n",
    "# === 9. Salvar resultado ===\n",
    "df.to_excel(\"produtos_com_marcas.xlsx\", index=False)\n",
    "\n",
    "# === 10. Relat√≥rio simples ===\n",
    "print(\"‚úÖ Processamento conclu√≠do!\")\n",
    "print(f\"Total de produtos analisados: {len(df_filtrado)}\")\n",
    "print(\n",
    "    f\"Marcas detectadas automaticamente: {df_filtrado['Marca Detectada'].notna().sum()}\"\n",
    ")\n",
    "\n",
    "# Top 10 marcas mais detectadas\n",
    "print(\"\\nTop marcas detectadas:\")\n",
    "print(df_filtrado[\"Marca Detectada\"].value_counts().head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f84477b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_excel(\"pai_filho_variantes.xlsx\")\n",
    "\n",
    "\n",
    "# --- Fun√ß√£o para normalizar ---\n",
    "def normalizar(texto):\n",
    "    texto = str(texto).lower().strip()\n",
    "    texto = (\n",
    "        unicodedata.normalize(\"NFKD\", texto).encode(\"ascii\", \"ignore\").decode(\"utf-8\")\n",
    "    )\n",
    "    texto = re.sub(r\"\\s+\", \" \", texto)\n",
    "    return texto\n",
    "\n",
    "\n",
    "df[\"chave_norm\"] = df[\"chave\"].apply(normalizar)\n",
    "\n",
    "\n",
    "# --- Fun√ß√£o para encontrar a base e varia√ß√£o dentro de um grupo ---\n",
    "def extrair_base_variacao(grupo_df):\n",
    "    textos = grupo_df[\"chave_norm\"].tolist()\n",
    "    palavras = [set(t.split()) for t in textos]\n",
    "\n",
    "    comuns = set.intersection(*palavras)\n",
    "    bases, variacoes = [], []\n",
    "    for texto in textos:\n",
    "        palavras_texto = texto.split()\n",
    "        variacao = [p for p in palavras_texto if p not in comuns]\n",
    "        base = [p for p in palavras_texto if p in comuns]\n",
    "        bases.append(\" \".join(base))\n",
    "        variacoes.append(\" \".join(variacao))\n",
    "\n",
    "    grupo_df[\"base\"] = bases\n",
    "    grupo_df[\"variacao\"] = variacoes\n",
    "    return grupo_df\n",
    "\n",
    "\n",
    "# --- Aplica por grupo ---\n",
    "df_resultado = (\n",
    "    df.groupby(\"ID_Variacao\", group_keys=False)\n",
    "    .apply(lambda g: extrair_base_variacao(g.copy()))\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "df_resultado.to_excel(\"base_nome.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ab5933",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471a3ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from rapidfuzz import fuzz\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# --- Fun√ß√£o para normalizar textos ---\n",
    "def normalizar(texto):\n",
    "    if pd.isna(texto):\n",
    "        return \"\"\n",
    "    texto = str(texto).upper().strip()\n",
    "    texto = \"\".join(\n",
    "        c\n",
    "        for c in unicodedata.normalize(\"NFD\", texto)\n",
    "        if unicodedata.category(c) != \"Mn\"\n",
    "    )\n",
    "    return texto\n",
    "\n",
    "\n",
    "def limpar_texto(txt):\n",
    "    if not isinstance(txt, str):\n",
    "        return \"\"\n",
    "    txt = unicodedata.normalize(\"NFD\", txt)\n",
    "    txt = txt.encode(\"ascii\", \"ignore\").decode(\"utf-8\")\n",
    "    txt = txt.upper()\n",
    "    txt = re.sub(r\"[^A-Z0-9 ]\", \"\", txt)\n",
    "    txt = re.sub(r\"\\s+\", \" \", txt).strip()\n",
    "    return txt\n",
    "\n",
    "\n",
    "# --- Carrega o Excel ---\n",
    "df = pd.read_excel(\"pai_filho_variantes.xlsx\")\n",
    "df = df[0:20]\n",
    "\n",
    "\n",
    "# --- Cria chave composta ---\n",
    "df[\"Chave\"] = df[\"Descri√ß√£o\"].apply(normalizar)\n",
    "\n",
    "# --- Ordena ---\n",
    "df = df.sort_values(by=[\"ID_Variacao\", \"Tipo\"], ascending=[True, True]).reset_index(\n",
    "    drop=True\n",
    ")\n",
    "\n",
    "# --- Fun√ß√£o com controle de sensibilidade ---\n",
    "\n",
    "\n",
    "def parte_comum(strings, sensibilidade):\n",
    "    if not strings:\n",
    "        return \"\"\n",
    "    base = strings[0]\n",
    "    for s in strings[1:]:\n",
    "        s = limpar_texto(s)\n",
    "        tokens_base = set(base.upper().split())\n",
    "        tokens_s = set(s.upper().split())\n",
    "\n",
    "        comum_tokens = tokens_base & tokens_s\n",
    "        comum = \" \".join(comum_tokens)\n",
    "        proporcao = fuzz.token_set_ratio(base, s) / 100\n",
    "\n",
    "        if proporcao >= sensibilidade:\n",
    "            base = comum\n",
    "        else:\n",
    "            limite = max(1, int(len(comum_tokens) * sensibilidade))\n",
    "            base = \" \".join(list(comum_tokens)[:limite])\n",
    "\n",
    "    return base.strip()\n",
    "\n",
    "\n",
    "print(\"üß© Gerando colunas base e variante...\")\n",
    "df[\"Base\"] = \"\"\n",
    "df[\"Variante\"] = \"\"\n",
    "\n",
    "for gid, grupo in df.groupby(\"ID_Variacao\", group_keys=False):\n",
    "    chaves = grupo[\"Chave\"].tolist()\n",
    "    comum = parte_comum(chaves, 0.1)\n",
    "\n",
    "    for idx, linha in grupo.iterrows():\n",
    "        texto = linha[\"Chave\"]\n",
    "        variante = texto.replace(comum, \"\").strip()\n",
    "        df.at[idx, \"Base\"] = comum\n",
    "        df.at[idx, \"Variante\"] = variante\n",
    "\n",
    "df[[\"Base\", \"Variante\", \"ID_Variacao\"]]\n",
    "\n",
    "# # --- Salva ---\n",
    "# arquivo_saida = \"base_nome.xlsx\"\n",
    "# df.to_excel(arquivo_saida, index=False)\n",
    "\n",
    "# print(\"‚úÖ Agrupamento conclu√≠do com sucesso!\")\n",
    "# print(f\"üìÇ Arquivo salvo como: {arquivo_saida}\")\n",
    "# print(f\"üì¶ Total de grupos de varia√ß√µes criados: {grupo_id - 1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565d3878",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = \"CHAVE PHILLIPS CRV TOCO C/PI 1/4X11/2 FAMASTIL FERRAMENTAS MANUAIS E ACESSORIOS\"\n",
    "b = \"CHAVE FENDA TOCO CRV C/PI 3/16X11/2 FOX/FAMASTIL FERRAMENTAS MANUAIS E ACESSORIOS\"\n",
    "\n",
    "\n",
    "# print(f\"Similarity score: {fuzz.token_sort_ratio(a, b)}\")\n",
    "\n",
    "# print(process.extract(collection, scorer=fuzz.ratio))\n",
    "\n",
    "\n",
    "set_a = set(a.split())\n",
    "set_b = set(b.split())\n",
    "\n",
    "semelhantes = set_a.intersection(set_b)\n",
    "diferentes_a = set_a - set_b\n",
    "diferentes_b = set_b - set_a\n",
    "\n",
    "print(\"Iguais:\", semelhantes)\n",
    "print(\"S√≥ em A:\", diferentes_a)\n",
    "print(\"S√≥ em B:\", diferentes_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb27b941602401d91542211134fc71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import itertools\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def normalizar(texto):\n",
    "    if pd.isna(texto):\n",
    "        return \"\"\n",
    "    texto = str(texto).upper().strip()\n",
    "    texto = \"\".join(\n",
    "        c\n",
    "        for c in unicodedata.normalize(\"NFD\", texto)\n",
    "        if unicodedata.category(c) != \"Mn\"\n",
    "    )\n",
    "    texto = re.sub(r\"[^A-Z0-9 ]\", \"\", texto)\n",
    "    texto = re.sub(r\"\\s+\", \" \", texto).strip()\n",
    "    return texto\n",
    "\n",
    "\n",
    "def similaridade_media(strings):\n",
    "    \"\"\"Calcula a similaridade m√©dia do grupo\"\"\"\n",
    "    pares = list(itertools.combinations(strings, 2))\n",
    "    if not pares:\n",
    "        return 1.0\n",
    "    scores = [fuzz.token_set_ratio(a, b) / 100 for a, b in pares]\n",
    "    return sum(scores) / len(scores)\n",
    "\n",
    "\n",
    "def parte_comum(strings, sensibilidade=0.7):\n",
    "    \"\"\"Extrai tokens comuns entre descri√ß√µes, ignorando tamanhos e c√≥digos num√©ricos\"\"\"\n",
    "    if not strings:\n",
    "        return \"\"\n",
    "\n",
    "    # Tokeniza e normaliza\n",
    "    token_lists = [normalizar(s).split() for s in strings]\n",
    "    todas_palavras = set(sum(token_lists, []))\n",
    "\n",
    "    # Remove tokens que s√£o medidas ou n√∫meros (ex: 3/4, 48X50)\n",
    "    def eh_medida(p):\n",
    "        return bool(re.match(r\"^\\d+[Xx/]\\d+$\", p)) or p.isdigit()\n",
    "\n",
    "    todas_palavras = {p for p in todas_palavras if not eh_medida(p)}\n",
    "\n",
    "    contagem = Counter()\n",
    "    for palavra in todas_palavras:\n",
    "        for tokens in token_lists:\n",
    "            similar = any(fuzz.ratio(palavra, t) / 100 >= sensibilidade for t in tokens)\n",
    "            if similar:\n",
    "                contagem[palavra] += 1\n",
    "\n",
    "    limite = max(1, int(len(strings) * 0.8))\n",
    "    comuns = [w for w, c in contagem.items() if c >= limite]\n",
    "\n",
    "    primeira = token_lists[0]\n",
    "    base_ordenada = [\n",
    "        t\n",
    "        for t in primeira\n",
    "        if any(fuzz.ratio(t, w) / 100 >= sensibilidade for w in comuns)\n",
    "    ]\n",
    "\n",
    "    base = \" \".join(base_ordenada).strip()\n",
    "    base = re.sub(r\"\\b\\d+([Xx/]\\d+)?\\b\", \"\", base).strip()\n",
    "    base = re.sub(r\"\\s+\", \" \", base).strip()\n",
    "\n",
    "    return base\n",
    "\n",
    "\n",
    "# --- Carrega base ---\n",
    "df = pd.read_excel(\"pai_filho_variantes.xlsx\")\n",
    "df = df.head(20)\n",
    "\n",
    "\n",
    "# --- Normaliza chave ---\n",
    "df[\"Chave\"] = np.where(\n",
    "    df[\"Categoria\"].isin([\"CONEX√ïES ESGOTO\", \"CONEX√ïES √ÅGUA\"]),\n",
    "    df[\"Categoria\"].apply(normalizar) + \" \" + df[\"Descri√ß√£o\"].apply(normalizar),\n",
    "    df[\"Descri√ß√£o\"].apply(normalizar),\n",
    ")\n",
    "\n",
    "# --- Ordena ---\n",
    "df = df.sort_values(by=[\"ID_Variacao\", \"Tipo\"], ascending=[True, True]).reset_index(\n",
    "    drop=True\n",
    ")\n",
    "\n",
    "# --- Cria colunas ---\n",
    "df[\"Base\"] = \"\"\n",
    "df[\"Variante\"] = \"\"\n",
    "\n",
    "\n",
    "for gid, grupo in tqdm(df.groupby(\"ID_Variacao\", group_keys=False)):\n",
    "    chaves = grupo[\"Chave\"].tolist()\n",
    "    media = similaridade_media(chaves)\n",
    "\n",
    "    # Sensibilidade din√¢mica\n",
    "    if media >= 0.9:\n",
    "        sensibilidade = 0.85\n",
    "    elif media >= 0.8:\n",
    "        sensibilidade = 0.75\n",
    "    else:\n",
    "        sensibilidade = 0.65\n",
    "\n",
    "    comum = parte_comum(chaves, sensibilidade)\n",
    "\n",
    "    # Se o grupo tiver s√≥ um item, usa a descri√ß√£o completa como base\n",
    "    if len(grupo) == 1 or not comum:\n",
    "        comum = normalizar(grupo[\"Descri√ß√£o\"].iloc[0])\n",
    "\n",
    "    for idx, linha in grupo.iterrows():\n",
    "        texto = linha[\"Chave\"]\n",
    "        variante = texto.replace(comum, \"\").strip()\n",
    "        df.at[idx, \"Base\"] = comum\n",
    "        df.at[idx, \"Variante\"] = variante\n",
    "\n",
    "\n",
    "# --- Resultado ---\n",
    "df[[\"ID_Variacao\", \"Descri√ß√£o\", \"Base\", \"Variante\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97daddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from env import fornecedores_web_scraping\n",
    "\n",
    "df = pd.read_excel(\"final.xlsx\")\n",
    "\n",
    "\n",
    "def gtin(codigo):\n",
    "    return codigo.isdigit() and len(codigo) in [8, 12, 13, 14]\n",
    "\n",
    "\n",
    "df[\"GTIN_V√°lido\"] = df[\"C√≥digo de Barras\"].astype(str).apply(gtin)\n",
    "\n",
    "df_validos = df[df[\"GTIN_V√°lido\"]].copy()\n",
    "\n",
    "\n",
    "def escolher_prioritario(grupo):\n",
    "    if len(grupo) == 1:\n",
    "        return grupo\n",
    "    fornecedores = grupo[\"Fornecedor\"].tolist()\n",
    "\n",
    "    if any(f in fornecedores_web_scraping for f in fornecedores):\n",
    "        return grupo[grupo[\"Fornecedor\"].isin(fornecedores_web_scraping)].head(1)\n",
    "\n",
    "    return grupo.head(1)\n",
    "\n",
    "\n",
    "df_filtrado = df_validos.groupby(\"C√≥digo de Barras\", group_keys=False).apply(\n",
    "    escolher_prioritario\n",
    ")\n",
    "\n",
    "df_final = pd.concat([df_filtrado, df[~df[\"GTIN_V√°lido\"]]], ignore_index=True)\n",
    "\n",
    "df_final = df_final.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb322f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "\n",
    "df = pd.read_excel('../planilhas/outputs/final.xlsx')\n",
    "\n",
    "unicos = df['Base'].unique()\n",
    "\n",
    "for idx, linha in df.iterrows():\n",
    "    df.at[idx, \"Url Imagem 1\"] = item\n",
    "    df.at[idx, \"Url Imagem 2\"] = item\n",
    "    df.at[idx, \"Url Imagem 3\" ] = item\n",
    "\n",
    "    API_KEY = \"AIzaSyCReSBWT3XKO8M0g6C1NtJcQFtuRnApYQA\"\n",
    "    CX = \"532347d8c03cc4861\"\n",
    "    query = unicos\n",
    "\n",
    "    url = \"https://www.googleapis.com/customsearch/v1\"\n",
    "    params = {\n",
    "        \"q\": query,\n",
    "        \"cx\": CX,\n",
    "        \"key\": API_KEY,\n",
    "        \"searchType\": \"image\",\n",
    "        \"num\": 3\n",
    "    }\n",
    "\n",
    "\n",
    "res = requests.get(url, params=params)\n",
    "\n",
    "data = res.json()\n",
    "\n",
    "for item in data.get(\"items\", []):\n",
    "    print(item[\"link\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d19d7007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processamento feito para 0 bases. Continue amanh√£ para o restante.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Caminho do arquivo final parcial\n",
    "ARQ_INPUT = '../planilhas/outputs/final.xlsx'\n",
    "ARQ_OUTPUT = '../planilhas/outputs/final_com_imagens.xlsx'\n",
    "\n",
    "API_KEY = \"AIzaSyCReSBWT3XKO8M0g6C1NtJcQFtuRnApYQA\"\n",
    "CX = \"532347d8c03cc4861\"\n",
    "\n",
    "# Carrega o DataFrame\n",
    "if os.path.exists(ARQ_OUTPUT):\n",
    "    df = pd.read_excel(ARQ_OUTPUT)\n",
    "else:\n",
    "    df = pd.read_excel(ARQ_INPUT)\n",
    "    df[\"Url Imagem 1\"] = \"\"\n",
    "    df[\"Url Imagem 2\"] = \"\"\n",
    "    df[\"Url Imagem 3\"] = \"\"\n",
    "\n",
    "# Cria lista de bases √∫nicas que ainda n√£o t√™m imagens\n",
    "unicos = df.loc[df[\"Url Imagem 1\"] == \"\", \"Base\"].unique()\n",
    "\n",
    "# Define quantas requisi√ß√µes voc√™ quer fazer por dia (ex: 100)\n",
    "limite_dia = 100\n",
    "\n",
    "# Faz apenas at√© o limite di√°rio\n",
    "for base in unicos[:limite_dia]:\n",
    "    url = \"https://www.googleapis.com/customsearch/v1\"\n",
    "    params = {\n",
    "        \"q\": base,\n",
    "        \"cx\": CX,\n",
    "        \"key\": API_KEY,\n",
    "        \"searchType\": \"image\",\n",
    "        \"num\": 3\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        res = requests.get(url, params=params)\n",
    "        data = res.json()\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao buscar {base}: {e}\")\n",
    "        continue\n",
    "\n",
    "    urls = [item[\"link\"] for item in data.get(\"items\", [])]\n",
    "\n",
    "    # Atualiza todas as linhas que t√™m essa base\n",
    "    for idx in df.index[df[\"Base\"] == base]:\n",
    "        for i in range(3):\n",
    "            coluna = f\"Url Imagem {i+1}\"\n",
    "            df.at[idx, coluna] = urls[i] if i < len(urls) else \"\"\n",
    "\n",
    "    # Pequena pausa para evitar limite por segundo\n",
    "    time.sleep(0.1)\n",
    "\n",
    "# Salva o DataFrame atualizado\n",
    "df.to_excel(ARQ_OUTPUT, index=False)\n",
    "\n",
    "print(f\"Processamento feito para {min(len(unicos), limite_dia)} bases. Continue amanh√£ para o restante.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3295b843",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f48744d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "API_KEY = \"AIzaSyCReSBWT3XKO8M0g6C1NtJcQFtuRnApYQA\"\n",
    "CX = \"532347d8c03cc4861\"\n",
    "query = \"gato siam√™s\"\n",
    "\n",
    "url = \"https://www.googleapis.com/customsearch/v1\"\n",
    "params = {\n",
    "    \"q\": query,\n",
    "    \"cx\": CX,\n",
    "    \"key\": API_KEY,\n",
    "    \"searchType\": \"image\",\n",
    "    \"num\": 3\n",
    "}\n",
    "\n",
    "res = requests.get(url, params=params)\n",
    "data = res.json()\n",
    "\n",
    "for item in data.get(\"items\", []):\n",
    "    print(item[\"link\"])  # URL da imagem\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
